---
title: "Tidymodels in R: Building tidy machine learning models"
date: "`r Sys.Date()`"
output: html_document
author: "Jen Cheng"
---

# About Data Analysis Report

This RMarkdown file contains the report of the data analysis done for the project on tidymodels in R: Building tidy machine learning models. It contains analysis such as data exploration, summary statistics and building the prediction models. The final report was completed on `r date()`. 

**Context:**

Did you know that hospital readmissions within 30 days cost the U.S. healthcare system over $17 billion on average annually?

The overarching aim of this study is to estimate the effect of glycated hemoglobin levels on diabetic inpatient outcomes. The data analysis seeks to:

* Can HbA1c and clinical records of diabetic inpatients predict risk of readmission?

**Data Description:**

The data set for this project is an excerpt of the dataset provided during the Visual Automated Disease Analytics (VADA) summer school training, 2018. The VADA Summer School training dataset was derived from the Health Facts database (Cerner Corporation, Kansas City, MO, USA). This database contains clinical records from 130 participating hospitals across the USA. These clinical records contain information pertaining to 69,984 observations and 27 variables including patient encounter data, demographics, HbA1c levels, diagnostic testing and treatments, and patient outcomes. Data used were from 1999–2008 from a cohort of 130 hospitals, deidentified and trimmed to include only inpatient visits.

Strack B, DeShazo JP, Gennings C, et al. Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records. Biomed Res Int. 2014;2014:781670. doi:10.1155/2014/781670

# Task One: Getting set up and overview of the project

In this task, you will import the required packages and data for this project. You may need to install these packages if not installed. You can use the `install.packages()` function.

```{r message = F, warning = F}
## Import required packages



library(tidyverse)
library(tidymodels)
library(themis)
library(table1)
library(ggpubr)
library(broom)
library(ggfortify)
library(GGally)
library(PerformanceAnalytics)
library(car)
library(caret)
library(skimr)
library(discrim)
library(glmnet)
library(kknn)
library(naivebayes)
library(kernlab)
library(xgboost)
library(gridExtra)

## Load the data set
readmit_df <- read_csv("readmission_data.csv")

## Check the dimension of the data set
dim(readmit_df)

## Check the column names for the data set
names(readmit_df)

```

# Task Two: Describe and summarize the data 
In this task, you will describe the data by creating summary statistics using R functions.
```{r}
## Get a broad overview of the data
skim(readmit_df)

## Quick data pre-processing - 
readmit_df_pp <- readmit_df %>% 
  mutate(readmit_num = case_when(
          readmitted == "No" ~ 0,
          readmitted == "Yes" ~ 1)) %>% 
  ## Relocate the readmitted variable to the last column
  relocate(readmitted, .after= readmit_num) %>%
  ## Convert variable data types
  mutate_at(vars(!c(readmit_num, hospital_stay, patient_visits, num_medications, num_diagnosis)), as.factor)

## Check the internal structure of the data frame
glimpse(readmit_df_pp)

## Create the summary table
table1::table1(~.| readmitted, data=readmit_df_pp)
```

# Task Three: Explore the categorical data using visualizations
In this task, you will explore the categorical variables using appropriate visualizations. It is time to generate some graphs from the data to gain insights. We will plot the distribution for the target `readmitted` and some features – `race`, `sex`, `age`, and `HbA1c`.

```{r}
## Create the plot for readmission status.
ggplot(readmit_df_pp, aes(x="", y=readmitted, fill=readmitted)) + 
  geom_bar(stat= "identity", width=1) +
  coord_polar("y", start=0)

## Create plots for race, sex, age, and HbA1c.
p1 <- ggplot(readmit_df_pp, aes(x="", y=race, fill=race)) + 
  geom_bar(stat= "identity", width=1) +
  coord_polar("y", start=0)

p2 <- ggplot(readmit_df_pp, aes(x="", y=sex, fill=sex)) + 
  geom_bar(stat= "identity", width=1) +
  coord_polar("y", start=0)

p3 <- ggplot(readmit_df_pp, aes(x="", y=age, fill=age)) + 
  geom_bar(stat= "identity", width=1) +
  coord_polar("y", start=0)

p4 <- ggplot(readmit_df_pp, aes(x="", y=HbA1c, fill=HbA1c)) + 
  geom_bar(stat= "identity", width=1) +
  coord_polar("y", start=0)
## Arrange the graphs
grid.arrange(p1, p2, p3, p4, ncol = 2)

## Create a bar graph of HbA1c levels
ggplot(data=readmit_df_pp) + 
  geom_bar(mapping=aes(x=HbA1c))

readmit_df_pp %>%
  ggplot() +
  geom_bar(mapping=aes(x=HbA1c))

## Create a bar graph of HbA1c by readmission status
ggplot(data=readmit_df_pp) +
  geom_bar(mapping=aes(x=HbA1c, fill=readmitted))

```


# Task Four: Explore the numeric data using visualizations
In this task, you will explore the numeric variables using appropriate visualizations. It is time to generate some graphs from the data to gain insights. We will plot the distribution for the `num_medications` variable.

```{r}
## Create a boxplot for number of medications
ggplot(readmit_df_pp, aes(y=num_medications)) +
  stat_boxplot(geom="errorbar", width=0.3) +
  geom_boxplot() + 
  coord_flip() +
  labs(x="Number of medications", title="Boxplot of number of medications")
  
## Create a boxplot for number of medications by readmission status
ggplot(readmit_df_pp, aes(x=readmitted, y=num_medications)) +
  stat_boxplot(geom="errorbar", width=0.3) +
  geom_boxplot() + 
  ggtitle("Comparing the number of medications across readmission status")

## Create a histogram for number of medications
ggplot(readmit_df_pp, aes(x=num_medications)) + 
  geom_histogram(colour="black", fill="white") +
  labs(x="Number of medications", title="Histogram of number of medications")

```

Practice Activity One: Create data visualizations
In this optional practice task, you will explore the data using using appropriate visualizations.

```{r}
## Create a bar graph of diabetic medication change
ggplot(data=readmit_df_pp) +
  geom_bar(mapping=aes(x=diabetesMed))

## Create a bar graph of diabetic medication change by readmission status
ggplot(readmit_df_pp) +
  geom_bar(mapping= aes(x= diabetesMed, fill=readmitted))

## Create a boxplot for number of diagnosis by readmission status
ggplot(readmit_df_pp, aes(x=readmitted, y=num_diagnosis)) +
  stat_boxplot(geom="errorbar", width=0.3) +
  geom_boxplot() +
  ggtitle("Comparing the number of diagnosis across readmission status")

```

# Task Five: Check for data issues
In this task, you will check the data for issues such as multicollinearity.

```{r, warning=FALSE}
## Create a correlation matrix of the numeric variables
readmit_df_pp %>% 
  dplyr::select(num_medications, num_diagnosis, patient_visits) %>% 
  cor()

## Create a correlation matrix of the numeric variables
readmit_df_pp %>%
  dplyr::select_if(is.numeric) %>%
  ggcorr(label=T)

## Create a correlation chart of the numeric variables
readmit_df_pp %>%
  dplyr::select_if(is.numeric) %>%
  chart.Correlation()

## Calculate the variance inflation factor
car::vif(lm(readmit_num ~ num_medications + hospital_stay + num_diagnosis +
              patient_visits, data=readmit_df_pp)) %>%
  broom::tidy()

```

**Variance inflation factor (VIF)**

A VIF is a measure of the amount of multicollinearity in regression analysis. It is important to see if the continuous/numeric variables (features) are collinear.

**What Can VIF Tell You?**

* VIF equal to 1 = variables are not correlated

* VIF between 1 and 5 = variables are moderately correlated 

* VIF greater than 5 = variables are highly correlated [1].

# Task Six: Create data splits for modeling
In this task, you will split the data into training and testing sets and create a cross-validated split of the training set for parameter tuning.

```{r}
## Set the seed
set.seed(2024)

## Drop the readmit_num variable
readmit_df_pp <- readmit_df_pp %>% 
                  select(-readmit_num)

## Take a glimpse at the data
glimpse(readmit_df_pp)

## Create the data split
readmit_split <- initial_split(readmit_df_pp, prop=0.8, strata = readmitted)
readmit_split

## Create training and testing sets
readmit_train <- training(readmit_split)
readmit_test <- testing(readmit_split)

## Check the dimension
dim(readmit_train)
dim(readmit_test)

## Create CV object from training data
readmit_folds <- vfold_cv(readmit_train)
readmit_folds
```

# Task Seven: Create a recipe
In this task, you will create and define a recipe as part of the tidymodels workflow. 
```{r}
## Create a recipe
readmit_recipe <- 
  ## Specify the formula
  recipe(formula = readmitted ~., data=readmit_train) %>%
  
  ## Specify pre-processing steps
  step_normalize(all_numeric_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot=T) %>%
  step_zv(all_predictors())

readmit_recipe

## Extract the preprocessed data (not necessary for the pipeline)
readmit_train_preprocessed <- readmit_recipe %>%
  ## Apply the recipe to the training data
  prep(readmit_train) %>%
  ## Extract the pre-processed training data
  juice()

## Print the pre-processed data
readmit_train_preprocessed
```

# Task Eight: Specify the models
In this task, you will specify various machine learning models to fit.
```{r}
## Decision tree
decision_tree_rpart_spec <- 
  decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')


## Logistic classifier (with a glmnet engine)
logistic_reg_glmnet_spec <-
  logistic_reg(penalty=tune(), mixture=tune()) %>%
  set_engine('glmnet')


## Naive Bayes
naive_Bayes_naivebayes_spec <-
  naive_Bayes(smoothness=tune(), Laplace =tune()) %>%
  set_engine('naivebayes')


## K-Nearest Neighbours
nearest_neighbor_kknn_spec <-
  nearest_neighbor(neighbors=tune(),
                   weight_func = tune(), dist_power = tune()) %>%
  set_engine('kknn') %>%
  set_mode('classification')


## Random forest
rand_forest_ranger_spec <-
  rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine('ranger') %>%
  set_mode('classification')


## Linear Support Vector Machine (SVM)
svm_linear_kernlab_spec <-
  svm_linear(cost=tune(), margin=tune()) %>%
  set_engine('kernlab') %>%
  set_mode('classification')


## Radial Basis Function (RBF) kernel SVM
svm_rbf_kernlab_spec <-
  svm_rbf(cost=tune(), rbf_sigma=tune(), margin=tune()) %>%
  set_engine('kernlab') %>%
  set_mode('classification')


## XGBoost
xgboost_spec <-
  boost_tree(trees = tune(), mtry=tune(), learn_rate = tune()) %>%
  set_engine('xgboost') %>%
  set_mode('classification')


```

Practice Activity Two: Specify a random forest model
In this optional practice task, you will practice how to specify a random forest model with one parameter.

```{r}
## Specify that the model is a random forest
randforest_model <- 
  ## Specify that the `mtry` parameter needs to be tuned
  rand_forest(mtry=tune()) %>%
  ## Select the engine/package that underlies the model
  set_engine("ranger", importance= "impurity") %>%
  ## Choose a binary classification mode
  set_mode("classification")
```

# Task Nine: Create a workflow set
In this task, you will put the model and recipes together into a workflow using workflow_set() (from the workflows package).
```{r}
## Create the workflow set
readmit_workflow_set <- workflow_set(
  preproc = list(rec=readmit_recipe),
  models=list(decision_tree=decision_tree_rpart_spec,
              logistic_reg= logistic_reg_glmnet_spec,
              naive_bayes = naive_Bayes_naivebayes_spec,
              knn = nearest_neighbor_kknn_spec,
              random_forest = rand_forest_ranger_spec,
              svm_linear = svm_linear_kernlab_spec,
              svm_rbf = svm_rbf_kernlab_spec,
              xgboost = xgboost_spec)
)

## Print the workflow set
readmit_workflow_set
```

# Task Ten: Tune the hyperparameters
In this task, you will tune the hyperparameters of selected models using grid search.

```{r, error=FALSE}
## Setting up the control parameters
grid_ctrl <- control_grid(
  verbose = TRUE,
  save_pred = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE
)

## Define the metrics
readmit_metrics <- metric_set(accuracy, roc_auc, f_meas, sens,)

## Create parallel training
doParallel::registerDoParallel()

## Setting the start time
strt.time <- Sys.time()

## Tune the model 
#grid_results <- readmit_workflow_set %>% 
#   workflow_map(
#    verbose = TRUE,
#     seed = 2024,
#     resamples = readmit_folds,
#     grid = 7,
#     control = grid_ctrl,
#    metrics = readmit_metrics
#   )

## Tracking the duration of loop
Sys.time() - strt.time

## Stop parallel training 
doParallel::stopImplicitCluster()

## Save the grid results
#write_rds(grid_results, "readmit_grid_results.rds")

## Load the grid results
grid_results <- read_rds("readmit_grid_results.rds")
```

Practice Activity Three: Tune a machine learning model
In this optional practice task, you will practice how to tune a random forest model.

```{r}
## Specify a random model 
rf_model <- 
  rand_forest(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification") 

## Create the workflow
rf_workflow <- workflow() %>%
  ## Add the recipe
  add_recipe(readmit_recipe) %>%
  ## Add the model
  add_model(rf_model)

## Specify the values to try
rf_grid <- expand.grid(mtry = c(3, 4, 5))

## Define the metrics
rf_metrics <- metric_set(accuracy, roc_auc)

## Tune the model
rf_tune_results <- rf_workflow %>%
  tune_grid(resamples = readmit_folds,
            grid = rf_grid,
            metrics = rf_metrics)

## Print the results with collect_metrics()
rf_tune_results %>%
  collect_metrics()
```

# Task Eleven: Evaluate and select prediction models
In this task, you will evaluate and compare the performance of different models and select the best-performing one based on predefined criteria.

```{r}
## Create a table of model metric results
grid_results %>% 
  rank_results(select_best = TRUE) %>% 
  mutate(across(c("mean","std_err"), \(x) round(x, 3))) %>% 
  select(wflow_id, .metric, mean) %>% 
  pivot_wider(names_from = .metric, values_from = mean) %>% 
  arrange(-f_meas)

## Plot the best model
autoplot(grid_results, select_best = TRUE)
```

**Note:** Judging by the model results (such as the sensitivity, specificity, and F1-score), the linear SVM, naive Bayes, and [penalized] logistic classifier model were among the best performing models.

The choice of which model to select depends on different factors. In this case, I decided to select the logistic classifier model for several reasons:

* Although the linear SVM, naive Bayes, and logistic regression had comparable metric results; however, I'd strike out the naive Bayes model because the specificity is really low - it will perform badly in predicting no readmissions.

* Also, several empirical and simulation studies have shown that *"Logistic classifier was comparable to complex machine learning models in discrimination, but was superior to these algorithms in calibration overall"* [2-5].

* Usually, the challenge with logistic classifier is when the number of predictors (p) is greater than the number of observations (n); but that is not the case here.

* In addition, the logistic classifier is simple to use, understand, and interpret by non-technical people, including clinicians.

With these evidences, we pull the best parameters from the logistic classifier model and finalize the workflow.


# Task Twelve: Finalize the workflow
In this task, you will pull the best parameters from the best model and finalize the workflow.
```{r}
## Select the best model
best_results <- grid_results %>%
 workflow_map(grid_results, "fit_resamples")  # Example of extracting results
   #extract_workflow_set_results("rec_logistic_reg") %>%
  select_best(metric= "f_meas")

## Print the best model
best_results

## Finalize the workflow using best model
final_wf <- grid_results %>%
  extract_workflow("rec_logistic_reg") %>%
  finalize_workflow(best_results)

final_wf
```


Practice Activity: Finalize the workflow for a machine learning model
In this optional practice task, you will practice how to finalize the workflow for a random forest model.

```{r}
## Specify a random forest model 
rf_model <- 
  rand_forest(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification") 

## Create the workflow
rf_workflow <- workflow() %>%
  add_recipe(readmit_recipe) %>%
  add_model(rf_model)

## Specify the values to try
rf_grid <- expand.grid(mtry = c(3, 4, 5))

## Define the metrics
rf_metrics <- metric_set(accuracy, roc_auc)

## Tune the model
rf_tune_results <- rf_workflow %>%
  tune_grid(resamples = readmit_folds,
            grid = rf_grid,
            metrics = rf_metrics)

## Select the best value for the mtry parameter using accuracy
rf_final <- rf_tune_results %>%
  select_best(metric = "accuracy")

## Print the best model
rf_final

## Finalize the workflow
rf_workflow_final <- rf_workflow %>%
  finalize_workflow(rf_final)
```

# Task Thirteen: Evaluate the model on the test set
In this task, you will assess the performance of the best model on the test set.
```{r}
## Fit on the training set and evaluate on test set
readmit_last_fit <- final_mf %>%
  last_fit(readmit_split, metrics=readmit_metrics)

## Print the result of the last_fit
readmit_last_fit

## Extract the performance of the last model fit
collect_metrics(readmit_last_fit)


## Generate predictions from the test set
test_preds <- readmit_last_fit %>%
  collect_predictions()

test_preds

## Create a confusion matrix
readmit_last_fit %>%
  collect_predictions() %>%
  conf_mat(estimate= .preds_class, truth= readmitted)

```

# Task Fourteen: Fitting and using the final model
In this task, you will train the final model on the full data set and then use it to predict the response for new data.
```{r}
## Fit the final model
final_model <- fit(final_wf, readmit_df_pp)

## Print the final model
final_model

## Predict on the first patient
predict(final_model, readmit_df_pp[1,] %>% select(-readmitted), type = "prob")

## Define the data for the new patient
new_patient <- tribble(~race, ~sex, ~age, ~hospital_stay, ~HbA1c, ~diabetesMed,
                       ~admit_source, ~patient_visits,~num_medications, 
                       ~num_diagnosis, ~insulin_level,
                       "Others", "Male", "<60 years", 7, "Normal", "No",
                       "Emerg", 3, 20, 8, "Up")
new_patient

## Predict the readmission status for the new patient
predict(final_model, new_data = new_patient)

## Save the final model
#write_rds(final_model,"final_readmission_model.rds")
```

## Cumulative Activity: Create a stroke prediction model

As a junior data scientist at a leading healthcare organization build a model to predict the likelihood of a patient suffering a stroke. The model could help improve a patient’s outcomes.To complete this cumulative activity, you will have to load the data, create data splits, create a recipe, specify a random forest model, create a workflow, and tune the parameters. Finally, you will finalize the workflow, evaluate the model on the test set and fit the final model.
```{r}
## Load the cleaned stroke data
stroke_data <- read_csv("stroke_data.csv")

## Take a glimpse at the data
glimpse(stroke_data)

## Convert data type
stroke_data <- stroke_data %>% 
  mutate_at(vars(!c(age, avg_glucose_level, bmi)), as.factor)

## Create a broad overview of the data set
glimpse(stroke_data)

## Create a correlation matrix for 
stroke_data %>%
  dplyr::select(age, avg_glucose_level, bmi) %>%
  ggcorr(label = TRUE)

## Set the seed
set.seed(2024)


## Create the data split


## Create training and testing sets


## Check the dimension


## Create the cross-validation set


## Define the recipe


## Specify the random forest model 


## Create the workflow


## Specify the values to try


## Define the metrics


## Tune the model



## Print the result using collect_metrics
stroke_tune_results %>%
  collect_metrics()

## Select the best value for the mtry parameter using the F-measure


## Print the best model


## Finalize the workflow


## Fit on the training set and evaluate on test set


## Extract the performance of the last model fit


## Create a confusion matrix


## Fit the final model on the entire data


## Extract the ranger model 
stroke_varimp <- extract_fit_parsnip(stroke_final_model)$fit
stroke_varimp

## Extract the variable importance scores
stroke_varimp$variable.importance
```

# References

1. Isixsigma. Variance Inflation Factor (VIF). Isixsigma website. https://www.isixsigma.com/dictionary/variance-inflation-factor-vif/. Published November 7, 2018. Accessed March 3, 2023.

2. Austin DE, Lee DS, Wang CX, et al. Comparison of machine learning and the regression-based EHMRG model for predicting early mortality in acute heart failure. Int J Cardiol. 2022;365:78-84. doi:10.1016/j.ijcard.2022.07.035

3. Lynam AL, Dennis JM, Owen KR, et al. Logistic regression has similar performance to optimised machine learning algorithms in a clinical setting: application to the discrimination between type 1 and type 2 diabetes in young adults. Diagn Progn Res. 2020;4:6. Published 2020 Jun 4. doi:10.1186/s41512-020-00075-2

4. Austin PC, Harrell FE Jr, Lee DS, Steyerberg EW. Empirical analyses and simulations showed that different machine and statistical learning methods had differing performance for predicting blood pressure. Sci Rep. 2022;12(1):9312. Published 2022 Jun 3. doi:10.1038/s41598-022-13015-5

5. Mišić VV, Rajaram K, Gabel EA. Simulation-based evaluation of machine learning models for clinical decision support: application and analysis using hospital readmission. npj Digit. Med. 4, 98 (2021). https://doi.org/10.1038/s41746-021-00468-7
